{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0g9vucaAzsQ_",
        "ZJqif5eg7srN",
        "kFvk9xn77aLM",
        "VKlAfu_8FRkw",
        "oLSHPNnHKsJY",
        "CpeaKe4IaC8Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ed0cbe65b664f499f15d1a21ba183e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ff33a188c414fc5a759dc65ec99b927",
              "IPY_MODEL_52e36a7958de4e58a812ee8789c4bcb2",
              "IPY_MODEL_46fd0372ce284fd28974f0c83dbb16f5"
            ],
            "layout": "IPY_MODEL_513d2b8c150d43f8a10cc55697dbd82c"
          }
        },
        "2ff33a188c414fc5a759dc65ec99b927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e171aef50a4be1b259705607d4f77f",
            "placeholder": "​",
            "style": "IPY_MODEL_1e56c394c69043668cdf6e07b4d1490c",
            "value": "(…)eta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf: 100%"
          }
        },
        "52e36a7958de4e58a812ee8789c4bcb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57355375dfd443c7859553d972ef8e36",
            "max": 4661212096,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bff271c3ae6942ef8efc814270360001",
            "value": 4661212096
          }
        },
        "46fd0372ce284fd28974f0c83dbb16f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de8a064da0ae4b8cab9c86df957f4539",
            "placeholder": "​",
            "style": "IPY_MODEL_8ec18beb32ec40c1a4d9b0dd3f7134b5",
            "value": " 4.66G/4.66G [00:46&lt;00:00, 37.3MB/s]"
          }
        },
        "513d2b8c150d43f8a10cc55697dbd82c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e171aef50a4be1b259705607d4f77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e56c394c69043668cdf6e07b4d1490c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57355375dfd443c7859553d972ef8e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff271c3ae6942ef8efc814270360001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de8a064da0ae4b8cab9c86df957f4539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ec18beb32ec40c1a4d9b0dd3f7134b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Langchain**"
      ],
      "metadata": {
        "id": "0g9vucaAzsQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is a framework designed to build applications using Large Language Models (LLMs) by integrating components like memory, chaining, retrieval, and agentic workflows. It simplifies tasks such as text generation, summarization, and question-answering by connecting LLMs with external data sources, APIs, and tools.\n",
        "\n",
        "**Key features**:\n",
        "- **Chains**: Sequences of LLM calls or other functions for structured execution.\n",
        "- **Memory**: Stores conversation history to maintain context.\n",
        "- **Retrieval**: Integrates with vector databases like FAISS or Pinecone for knowledge augmentation.\n",
        "- **Agents**: Uses tools dynamically to enhance decision-making.\n",
        "- **Integrations**: Supports OpenAI, Hugging Face, Cohere, Gemini, and more.\n",
        "\n",
        "LangChain is widely used for chatbots, RAG (Retrieval-Augmented Generation), and workflow automation. It streamlines LLM usage by handling API calls, formatting, and orchestration, making it a key tool for AI-driven applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "NDawkHKeyas6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this blog, we will look at the basic steps in using the model, prompts and output parsing. Let's learn a bit about these first -\n",
        "\n",
        "**1. Models**  \n",
        "LangChain supports multiple types of language models:  \n",
        "- **LLMs (Large Language Models):** OpenAI, Hugging Face, Cohere, LLaMA, GPT4All, Gemini, etc.  \n",
        "- **Chat Models:** Optimized for multi-turn conversations (e.g., OpenAI ChatGPT, Claude, Gemini).  \n",
        "- **Embedding Models:** Convert text into vector representations for similarity search (e.g., OpenAI Embeddings, SentenceTransformers).  \n",
        "- **Custom Models:** Allows fine-tuned or self-hosted models via API integration or local execution.  \n",
        "\n",
        "**2. Prompting**  \n",
        "- **PromptTemplates:** Structures prompts dynamically using placeholders (e.g., `f\"Summarize: {text}\"`).  \n",
        "- **Few-shot Prompting:** Provides examples within the prompt to improve response quality.  \n",
        "- **Retrieval-Augmented Prompting:** Fetches external knowledge before generating responses.  \n",
        "- **Parameterized Inputs:** Customizes prompts based on user input for personalization.  \n",
        "\n",
        "**3. Parsing**  \n",
        "- **Output Parsers:** Extract structured data (e.g., JSON, lists) from model responses.  \n",
        "- **Regex/Text Parsers:** Cleans and formats LLM output for specific applications.  \n",
        "- **Function Calling (Tool Use):** Directs LLMs to produce API calls, structured responses, or execute commands."
      ],
      "metadata": {
        "id": "43ejQhk6y_sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models refer to the language models that power many applications. A significant part of working with them involves designing inputs, known as prompting, to effectively communicate with the model. On the other end, parsing focuses on structuring the model's output into a more organized format for downstream tasks.  \n",
        "\n",
        "When building an application with large language models (LLMs), there will often be reusable components that repeatedly prompt the model, process its output, and handle related operations. This allows for a more efficient and structured approach, providing a set of abstractions that simplify these tasks."
      ],
      "metadata": {
        "id": "XEHjKi9Ch7Ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Gemini**"
      ],
      "metadata": {
        "id": "RYh-25B-zjAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini Model (by Google DeepMind)**  \n",
        "Gemini is a family of multimodal AI models developed by Google DeepMind, designed for text, images, audio, and code processing. Built on transformer-based architecture, it excels in reasoning, retrieval-augmented generation (RAG), and advanced problem-solving.  \n",
        "\n",
        "Key features:  \n",
        "- **Multimodal Capabilities:** Understands and generates text, images, and videos.  \n",
        "- **Integration with Google Tools:** Works with Search, Docs, and Workspace apps.  \n",
        "- **Variants:** Gemini 1, 1.5, and future iterations optimized for efficiency.  \n",
        "- **Fine-tuning & API Access:** Available via Google Cloud for enterprise use.  \n",
        "\n",
        "For the API access to Gemini, visit - https://aistudio.google.com/apikey and get the API key to get started.\n",
        "\n",
        "Gemini competes with OpenAI’s GPT-4 and is optimized for real-world applications like research, education, and business automation."
      ],
      "metadata": {
        "id": "oa2KHaCYzucj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model Setup and Configuration**"
      ],
      "metadata": {
        "id": "xnUiGzZhi6Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of below code block -\n",
        "- Imports required libraries (os for environment variables and google.generativeai for Gemini API).\n",
        "- Configures the Gemini API by retrieving the API key from the environment (GEMINI_API_KEY)."
      ],
      "metadata": {
        "id": "HIr1fso_4dnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
      ],
      "metadata": {
        "id": "i93f-bbGissA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Defines a dictionary generation_config to configure the model’s behavior, setting parameters like:\n",
        "  - temperature (0.2) → Controls randomness (lower = more deterministic).\n",
        "  - top_p (0.95) & top_k (40) → Control sampling diversity.\n",
        "  - max_output_tokens (8192) → Limits response length.\n",
        "  - response_mime_type → Ensures text output format.\n",
        "\n",
        "- Initializes the llm_model as gemini-1.5-flash with the defined configuration."
      ],
      "metadata": {
        "id": "kT2hBCRE4jdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "generation_config = {\n",
        "  \"temperature\":0.2,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 40,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "\n",
        "llm_model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        ")\n"
      ],
      "metadata": {
        "id": "wWbC2pgajZc-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above codes sets up the Gemini model with specified generation settings, ensuring controlled, structured outputs."
      ],
      "metadata": {
        "id": "2P7ILBiX43jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Defines get_completion(prompt, model=llm_model) to send a prompt to Gemini and return the response.\n",
        "\n",
        "- Calls get_completion(\"What is 1+1?\") and prints the response and token usage."
      ],
      "metadata": {
        "id": "2khAXKw6jir1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model Usage in simplest format**"
      ],
      "metadata": {
        "id": "ZJqif5eg7srN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=llm_model):\n",
        "\n",
        "    response = llm_model.generate_content(\n",
        "        prompt\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "response = get_completion(\"What is 1+1?\")"
      ],
      "metadata": {
        "id": "HTHxINFAjNy4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Response is : ', response.candidates[0].content.parts[0])\n",
        "print('Token Usage : ', response.usage_metadata.total_token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK3w47e0lJX7",
        "outputId": "6fc1909f-2177-4900-8138-abe73723ecfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response is :  text: \"1 + 1 = 2\\n\"\n",
            "\n",
            "Token Usage :  15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's say we have a customer email and we want to convert it into a predefined -\n",
        "- Define a customer_email containing a complaint.\n",
        "- Specify a style variable (American English in an angry, rude tone used here).\n",
        "- Constructs a formatted prompt instructing Gemini to rewrite customer_email in the defined style.\n",
        "- Calls get_completion(prompt) and prints the transformed response with token usage."
      ],
      "metadata": {
        "id": "VW6TbAgZloXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"\"\"\n",
        "Our server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933. The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM and troubleshoot the network issue affecting the iLO.\"\"\"\n",
        "\n",
        "\n",
        "style = \"\"\"American English in a very angry and rude tone\"\"\""
      ],
      "metadata": {
        "id": "N975enZQj8J4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)  # This is how the prompt looks with your defined style and customer mail"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h3aol-xlzvI",
        "outputId": "29f7e5e0-dbf0-4ac1-88da-64fce4da4580"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is American English in a very angry and rude tone.\n",
            "text: ```\n",
            "Our server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933. The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM and troubleshoot the network issue affecting the iLO.```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_1 = get_completion(prompt)"
      ],
      "metadata": {
        "id": "nQtrNFZglzrs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Response is : ', response_1.candidates[0].content.parts[0])\n",
        "print('\\n')\n",
        "print('Token Usage : ', response_1.usage_metadata.total_token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-A5MrAhl-Ij",
        "outputId": "ac5c2764-6b3e-4cbf-9b17-06becf384847"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response is :  text: \"ARE YOU KIDDING ME?!  That WORTHLESS GX-780 SERVER is DOWN AGAIN!  DIMM slot 4 is FRIED – a freakin\\' 16GB-DDR4-2933 piece of junk! And the iLO?  It\\'s decided to take a nap, with a pathetic 50% packet loss on port 2!  Get your sorry butts over here NOW and fix this garbage before I personally replace every single one of you with a toaster oven that\\'s probably more reliable!  We need someone ON-SITE, and we need them YESTERDAY!\\n\"\n",
            "\n",
            "\n",
            "\n",
            "Token Usage :  249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer mail is now changed to a very rude style as you can read from the above. Can the same prompt convert the texts if in other language? Let's see -"
      ],
      "metadata": {
        "id": "yQcKcpDXmYSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Email is stated in Bengali language now\n",
        "customer_email_in_bengali = \"\"\"আমাদের সার্ভার, মডেল GX-780, গুরুতর ডাউনটাইমের সম্মুখীন হচ্ছে। আমরা স্লট 4-এ, বিশেষ করে পার্ট নম্বর 16GB-DDR4-2933-এ মেমরি DIMM ত্রুটি দেখতে পাচ্ছি। iLO একটি নেটওয়ার্ক সংযোগ ব্যর্থতার রিপোর্ট করছে, পোর্ট 2-এ 50% প্যাকেট ক্ষতির হার রয়েছে। ত্রুটিপূর্ণ DIMM প্রতিস্থাপন এবং iLO-কে প্রভাবিত করে এমন নেটওয়ার্ক সমস্যা সমাধানের জন্য আমাদের তাৎক্ষণিকভাবে সাইট সহায়তা প্রয়োজন।\"\"\"\n",
        "\n",
        "style = \"\"\"American English in a very soft and polite tone\"\"\"  # Now we are using the style of soft and polite"
      ],
      "metadata": {
        "id": "rgAwaOWImy1D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{customer_email_in_bengali}```\n",
        "\"\"\"\n",
        "\n",
        "response_2 = get_completion(prompt)\n",
        "\n",
        "\n",
        "print('Response is : ', response_2.candidates[0].content.parts[0])\n",
        "print('\\n')\n",
        "print('Token Usage : ', response_2.usage_metadata.total_token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "TnDNb-APmyxj",
        "outputId": "bf0b6229-e174-4aca-ad08-a1239d67158e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response is :  text: \"Our server, model GX-780, is experiencing a significant outage.  We\\'re seeing a memory DIMM error in slot 4, specifically part number 16GB-DDR4-2933.  The iLO is reporting a network connectivity failure, with a 50% packet loss rate on port 2. We would be very grateful for immediate on-site assistance to replace the faulty DIMM and resolve the network issue affecting the iLO.\\n\"\n",
            "\n",
            "\n",
            "\n",
            "Token Usage :  341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini can translate languages using this method, but check the documentation for supported languages. Till now, we have seen defining a function for generating responses, tested it with a simple query, and then processed a customer complaint by reformatting it into a specific style."
      ],
      "metadata": {
        "id": "GR450753nLLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Gemini with Langchain**"
      ],
      "metadata": {
        "id": "kFvk9xn77aLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using LangChain for the above tasks simplifies API calls, prompt management, and response parsing. It enables structured workflows, caching, and memory for conversation history, improving efficiency. LangChain can enhance this use case by integrating **retrieval-augmented generation (RAG)** for knowledge-based responses, **output parsers** for structured results, and **agents/tools** for dynamic prompt adjustments. It also supports **multi-model orchestration**, allowing seamless switching between Gemini and other LLMs."
      ],
      "metadata": {
        "id": "WkzRW5ok7ZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will need to install the google genai langchain library\n",
        "#!pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "id": "_4s_1wRtnj87"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Imports GoogleGenerativeAI from LangChain, sets GEMINI_API_KEY, and initializes the Gemini gemini-1.5-flash model with temperature 0 for deterministic responses."
      ],
      "metadata": {
        "id": "vJn2rGMb87ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain with Gemini\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import os"
      ],
      "metadata": {
        "id": "8J4I13xTnaJ1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY   # This has to be done for the model to recognize the API Key for Gemini\n",
        "\n",
        "# Try running the next block without setting this API. You will get error when you will invoke the model\n",
        "# If you set the API and then run the next block, you will see the field of google_api_key=SecretStr('**********') in the llm model,\n",
        "# ensuring your model recognizes the API"
      ],
      "metadata": {
        "id": "woZA8grPuX-J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's initialize the model first\n",
        "\n",
        "llm = GoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0   # Setting temperature to 0 for definite output\n",
        ")\n",
        "\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wz7u5R3naGY",
        "outputId": "c162a349-b524-496c-df9d-4fc67ef14c69"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleGenerativeAI(model='gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7eb6c4fdde90>, default_metadata=()))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a template string that defines a prompt structure for an LLM. It instructs the model to translate text into a specific style. {style} and {text} are placeholders that will be replaced with actual values later. The text to be translated is enclosed within triple backticks (```) to ensure clarity and prevent misinterpretation by the model.\n",
        "\n",
        "It makes the prompt reusable and dynamic for different inputs. It helps in maintaining structure while customizing the style and text for various use cases."
      ],
      "metadata": {
        "id": "Ze1-hMOZriEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rPma19mBnaBC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPromptTemplate is a LangChain utility that formats prompts dynamically. It allows for parameterized prompt creation where different values can be inserted into predefined templates. It helps standardize prompt structures for LLM calls, and makes it easy to apply the same logic across multiple text transformations.\n",
        "\n",
        ".from_template(template_string) converts the template_string into a ChatPromptTemplate instance. This enables easy formatting by replacing {style} and {text} dynamically. The resulting object (prompt_template) can now be used to generate formatted prompts.\n",
        "\n",
        "This avoids manually constructing prompts for every input, and makes it scalable for various use cases like translation, summarization, or sentiment analysis."
      ],
      "metadata": {
        "id": "VjEfm6cI9_FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ],
      "metadata": {
        "id": "SGgeGW3Fj8HJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template  # This is how the prompt template instance for your template string looks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vZdB3Oer6Bn",
        "outputId": "a3ac3dbb-3b49-4622-bb0b-2dc71397db03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. \\ntext: ```{text}```\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdyNvffHr5-O",
        "outputId": "200e719b-e2b2-4bb1-8818-91dab2eecff5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. \\ntext: ```{text}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer Complaint Processing: Converts a pirate-themed complaint into British English in an angry tone using format_messages from the ChatPromptTemplate.\n",
        "\n",
        ".format_messages() replaces {style} and {text} in prompt_template with actual values:\n",
        "- customer_style: Defines the desired tone/style for the translation (e.g., \"British English in a very angry tone\").\n",
        "- customer_email: The text that needs to be transformed.\n",
        "\n",
        "The formatted output (customer_messages) is now ready to be sent to an LLM for processing. This automates text transformation in a structured manner, and ensures consistent, reproducible outputs for different inputs."
      ],
      "metadata": {
        "id": "wtuNz-vv9Ez1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_style = \"\"\"British English in a very angry tone\"\"\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Our server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933. The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM and troubleshoot the network issue affecting the iLO.\n",
        "\"\"\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(style = customer_style, text = customer_email)"
      ],
      "metadata": {
        "id": "oWfQBEoQr55N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_hmwk-Mison",
        "outputId": "df5206f0-77de-4e6c-c710-235313f37504"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is British English in a very angry tone. \\ntext: ```\\nOur server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933. The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM and troubleshoot the network issue affecting the iLO.\\n```\\n\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G84hp5d1swJN",
        "outputId": "45178611-9d5f-4c44-8563-c56f1f774c9a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sending it to the LLM for processing via llm.invoke() -"
      ],
      "metadata": {
        "id": "jdnfD-nk9Sln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response = llm.invoke(customer_messages)"
      ],
      "metadata": {
        "id": "XSqS3tv-swF2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "d_fVohUbswDP",
        "outputId": "b1c834d2-f2b4-471c-d166-064334f8db80"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Right, listen here, you lot!  Our blasted GX-780 server's gone belly up!  We've got catastrophic downtime, thanks to a flipping memory DIMM error on slot four – the 16GB-DDR4-2933 blighter, specifically!  And the iLO?  It's decided to have a complete hissy fit, reporting a network connection failure with a scandalous 50% packet loss on port two!  We need someone, *anyone*, on-site this instant to replace that infernal DIMM and sort out this bloody network mess before I lose my utterly livid mind!  Get a move on!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we will perform the Service Reply Transformation: A customer service response is rewritten in a teasing, joking tone using the same prompt template."
      ],
      "metadata": {
        "id": "myWAqPL99Uic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"Thank you for reporting the server issues on your GX-780. We understand the severity of the downtime. Our technicians are currently investigating the DIMM error on slot 4, part 16GB-DDR4-2933, and the network issues affecting the iLO port 2. We've dispatched a technician to your location with replacement DIMMs. They'll also run diagnostics on the network connectivity to isolate and resolve the packet loss. We'll provide updates every 30 minutes until the server is fully operational.\n",
        "\"\"\"\n",
        "\n",
        "service_style = \"\"\"a teasing and joking tone that speaks in English\"\"\"\n",
        "\n",
        "\n",
        "service_messages = prompt_template.format_messages(style = service_style, text = service_reply)\n",
        "\n",
        "\n",
        "service_response = llm.invoke(service_messages)"
      ],
      "metadata": {
        "id": "UsdsjX50ugVY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(service_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYjB1xQyugSB",
        "outputId": "26fdc5ab-e110-4030-acd7-f65d4f007b14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, honey, your GX-780 decided to throw a little hissy fit, huh?  We heard the screams – the *digital* screams, that is.  Turns out, it's got a case of the \"DIMM-s\" (get it?  DIMM-s?  Because it's a DIMM error?!  Okay, I'll stop).  Slot 4 is feeling a little left out, apparently, and that 16GB-DDR4-2933 stick is staging a full-blown rebellion.  And the iLO port 2?  Drama queen.  Total packet loss.  The whole thing's a soap opera.\n",
            "\n",
            "But don't you worry your pretty little head! We've sent in the cavalry – or, you know, a technician.  They're bringing backup DIMMs (because apparently, one rebellious stick wasn't enough drama).  They'll also give that network connection a good talking-to.  We'll keep you updated every 30 minutes, so you can live-tweet this whole tech-support saga.  Stay tuned for more exciting developments!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Using Output Parser of Langchain**"
      ],
      "metadata": {
        "id": "VKlAfu_8FRkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's say we want output in the form of JSON or dictionary that will have the key as our important data to search for and the value as the value reported in the mail. We can create a broad template for the same as below -"
      ],
      "metadata": {
        "id": "1m0FN0D3DsGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "Our server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933.\\\n",
        "The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM\\\n",
        "and troubleshoot the network issue affecting the iLO.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "model: Is any model name specified? \\Answer the model name directly if yes, False if not or unknown.\n",
        "\n",
        "issue_count: How many issues are reported for the product? If this information is found then give the number, else -1.\n",
        "\n",
        "parts_reported: Which are all the part names that got reported? Give all the names. If no part names found then reply with NA.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "model\n",
        "issue_count\n",
        "parts_reported\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Passing the review_template in the ChatPromptTemplate to create an instance of it\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "\n",
        "# Passing the customer review in the prompt template instance with the text as the placeholder\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "\n",
        "\n",
        "# Invoking model\n",
        "response_3 = llm.invoke(messages)"
      ],
      "metadata": {
        "id": "PSxsnmMwu-V2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REk2GVSaugPC",
        "outputId": "db557bd7-dc29-4963-dedf-0712570954d2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"model\": \"GX-780\",\n",
            "  \"issue_count\": 2,\n",
            "  \"parts_reported\": [\"16GB-DDR4-2933\", \"iLO\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(response_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7lmL-Q8ugMA",
        "outputId": "215f899f-e7b2-46eb-ee91-b31dcd3a933f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output is in string and not in JSON or dictionary format. So we need to do some work around here."
      ],
      "metadata": {
        "id": "Ra4amj2TEzXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ResponseSchema: Defines the expected output structure by specifying what fields to extract and their descriptions.\n",
        "\n",
        "- StructuredOutputParser: Ensures that the output follows the defined schema and is formatted correctly."
      ],
      "metadata": {
        "id": "F7vMgObsGOvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "\n",
        "\n",
        "# Defines a schema named \"model\". The description instructs the LLM to extract the model name if mentioned in the input text. If no model name is found, it should return \"False\".\n",
        "model_schema = ResponseSchema(name=\"model\",\n",
        "                             description=\"Is any model name specified? Answer the model name directly if yes, False if not or unknown.\")\n",
        "\n",
        "issue_count_schema = ResponseSchema(name=\"issue_count\",\n",
        "                                      description=\"How many issues are reported for the product? If this information is found then give the number, else -1.\")\n",
        "\n",
        "parts_reported_schema = ResponseSchema(name=\"parts_reported\",\n",
        "                                    description=\"This tells about the issues reported in the text? Give all the names. If no part names found then reply with NA.\")\n",
        "\n",
        "\n",
        "# Stores the defined schemas in a list to be used for structured parsing\n",
        "response_schemas = [model_schema,\n",
        "                    issue_count_schema,\n",
        "                    parts_reported_schema]\n",
        "\n",
        "\n",
        "# Creates an instance of StructuredOutputParser, ensuring that the extracted information follows the defined schema\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "\n",
        "# Generates formatting instructions for the LLM to structure its output properly\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xec3zT6wiu1",
        "outputId": "9179521e-1ea9-4266-ad00-ccbef7c84340"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"model\": string  // Is any model name specified? Answer the model name directly if yes, False if not or unknown.\n",
            "\t\"issue_count\": string  // How many issues are reported for the product? If this information is found then give the number, else -1.\n",
            "\t\"parts_reported\": string  // This tells about the issues reported in the text? Give all the names. If no part names found then reply with NA.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Defines a prompt template that instructs the LLM on how to extract the required details.\n",
        "\n",
        "- Uses {text} as a placeholder for the input text (product review).\n",
        "\n",
        "- Uses {format_instructions} to enforce structured output formatting."
      ],
      "metadata": {
        "id": "gCAbB87NGun7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "model: Is any model name specified? Answer the model name directly if yes, False if not or unknown.\n",
        "\n",
        "issue_count: How many issues are reported for the product? If this information is found then give the number, else -1.\n",
        "\n",
        "parts_reported: Tell the issues reported? Give all the names. If no part names found then reply with NA.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Converts the review_template_2 string into a ChatPromptTemplate, making it reusable for different inputs\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "\n",
        "\n",
        "# Fills in {text} with the actual customer review (customer_review)\n",
        "# Inserts the format instructions into {format_instructions}\n",
        "# Generates a properly formatted LLM input message\n",
        "messages = prompt.format_messages(text=customer_review,\n",
        "                                format_instructions=format_instructions)\n",
        "\n",
        "\n",
        "response_4 = llm.invoke(messages)"
      ],
      "metadata": {
        "id": "2ARTjMtAwirr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptth4S0Kwioo",
        "outputId": "d63fe807-79bb-44df-cca8-05014dcbac73"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"model\": \"GX-780\",\n",
            "  \"issue_count\": \"2\",\n",
            "  \"parts_reported\": \"memory DIMM, iLO\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = output_parser.parse(response_4)\n",
        "\n",
        "print(output_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKDJefSqislV",
        "outputId": "40b6deaf-87fd-4f62-d641-06990023ae5d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'GX-780', 'issue_count': '2', 'parts_reported': 'memory DIMM, iLO'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(output_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgaSx79ixqZp",
        "outputId": "3803c1af-bae7-45e8-d7c3-0e940b2bba56"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus we got the dictionary type as output!"
      ],
      "metadata": {
        "id": "MHU7nsOgHUIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Open Source Model**"
      ],
      "metadata": {
        "id": "KGsoUtX_HbNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might not have an API key to use Gemini, but no worries! There are plenty of open-source models available. While their output may not be as refined as Gemini or GPT, they can still deliver accurate results. With proper model tuning, you can enhance their performance and achieve more precise, targeted outputs.\n",
        "\n",
        "Every concept is remaining the same. We are just using the llama instruct model from GPT4All. What's GPT4All now?\n",
        "\n",
        "GPT4All is an open-source, locally runnable language model designed for accessibility and efficiency. It enables users to run large language models (LLMs) on personal devices without requiring cloud-based APIs. It supports multiple models, including Meta’s LLaMA, MPT, Falcon, and GPT-J, optimized for different use cases.\n",
        "\n",
        "Key features of GPT4All:\n",
        "- Local Execution: Runs entirely on a personal machine without internet dependency.\n",
        "- Privacy-Focused: No external data transmission, ensuring data security.\n",
        "- Customizable & Extendable: Supports fine-tuning and model customization.\n",
        "- Lightweight & Efficient: Optimized for consumer-grade hardware.\n",
        "\n",
        "So, GPT4All is ideal for users who need local, private, and free AI models, whereas OpenAI's GPT models offer superior performance, cloud accessibility, and commercial-grade capabilities."
      ],
      "metadata": {
        "id": "rTnhFBTWHeLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to load the model first. I am picking Llama instruct model. You need to perform !pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "qmjO6_RQYLU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Using Llama from GPT4All Directly**"
      ],
      "metadata": {
        "id": "oLSHPNnHKsJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "-Am3OLJUIr7y"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "2ed0cbe65b664f499f15d1a21ba183e5",
            "2ff33a188c414fc5a759dc65ec99b927",
            "52e36a7958de4e58a812ee8789c4bcb2",
            "46fd0372ce284fd28974f0c83dbb16f5",
            "513d2b8c150d43f8a10cc55697dbd82c",
            "c7e171aef50a4be1b259705607d4f77f",
            "1e56c394c69043668cdf6e07b4d1490c",
            "57355375dfd443c7859553d972ef8e36",
            "bff271c3ae6942ef8efc814270360001",
            "de8a064da0ae4b8cab9c86df957f4539",
            "8ec18beb32ec40c1a4d9b0dd3f7134b5"
          ]
        },
        "id": "RS0zGtBEwpfD",
        "outputId": "411cad20-3835-415d-848a-f302a759d5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)eta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ed0cbe65b664f499f15d1a21ba183e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm_model = Llama.from_pretrained(\n",
        "\trepo_id=\"GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k-GGUF\",\n",
        "\tfilename=\"Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf\",\n",
        "  n_ctx_per_seq = 128000,\n",
        "  verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching the completion prompt varies slightly depending on the model. The response generation process differs across models, so it's important to refer to the documentation for specifics. For LLaMA models in GPT4All, the structure is quite similar to OpenAI's GPT models."
      ],
      "metadata": {
        "id": "PrD7LwgVY0qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=llm_model):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = model.create_chat_completion(\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "brXkUSruYUHA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"What is 1+1?\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ow1YPA1OZQxE",
        "outputId": "e5ed9540-1c00-48da-f885-2996b1a5c2ed"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 + 1 = 2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"Can you work for me? Tell me within 10 words\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n73756BFaNRr",
        "outputId": "3823fc56-3968-4a38-8515-cc56907b0cac"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I can provide information, answer questions, and assist with tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"\"\"\n",
        "Our server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933. The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM and troubleshoot the network issue affecting the iLO.\n",
        "\"\"\"\n",
        "\n",
        "style = \"\"\"American English in a very soft tone\"\"\""
      ],
      "metadata": {
        "id": "3Zk1g7q7ZQtq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Sd1jpTiUZQqr",
        "outputId": "fab57d56-db2a-4905-b727-cc55ae71cb64"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here is the translated text in a soft tone and American English style:\\n\\n```\\nOur server, the model GX-780, is having a bit of a rough day. We're noticing some memory issues on slot 4, specifically with a 16GB DDR4 memory module that's just not cooperating. The iLO (that's our remote monitoring system, for those who don't know) is also giving us some trouble, with a network connection that's just not quite working right - we're seeing a 50% packet loss rate on port 2. We could really use some help getting this sorted out, so we're hoping someone can come on-site to take a look and see what's going on. Maybe we just need to swap out that one memory module and get everything up and running smoothly again.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Email is stated in Bengali language now\n",
        "customer_email_in_bengali = \"\"\"আমাদের সার্ভার, মডেল GX-780, গুরুতর ডাউনটাইমের সম্মুখীন হচ্ছে। আমরা স্লট 4-এ, বিশেষ করে পার্ট নম্বর 16GB-DDR4-2933-এ মেমরি DIMM ত্রুটি দেখতে পাচ্ছি। iLO একটি নেটওয়ার্ক সংযোগ ব্যর্থতার রিপোর্ট করছে, পোর্ট 2-এ 50% প্যাকেট ক্ষতির হার রয়েছে। ত্রুটিপূর্ণ DIMM প্রতিস্থাপন এবং iLO-কে প্রভাবিত করে এমন নেটওয়ার্ক সমস্যা সমাধানের জন্য আমাদের তাৎক্ষণিকভাবে সাইট সহায়তা প্রয়োজন।\"\"\"\n",
        "\n",
        "style = \"\"\"American English in a very funny tone\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nPwx4NMgL1DE",
        "outputId": "ae853192-8647-445a-9150-9c1aba483483"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```\\nOH NOEZ, our fancy server, the GX-780, is DOWN FOR THE COUNT! We\\'re talkin\\' critical downtime, folks! It\\'s like, the ultimate bummer. And the reason? Well, it\\'s not because we ate all the donuts in the break room (although, let\\'s be real, that\\'s a pretty good reason). Nope, it\\'s because of a pesky memory DIMM error on slot 4. Specifically, it\\'s that fancy 16GB-DDR4-2933 thingy that\\'s just not cooperating. And if that wasn\\'t enough, our iLO (that\\'s \"intelligent\" for you non-techies) is all like, \"Uh, hello? I\\'m trying to connect to the network over here, but it\\'s not working out.\" And the cherry on top? A 50% packet loss rate on port 2. Yeah, that\\'s just peachy. So, what\\'s the plan? We need some superhero on-site support to come and save the day (or at least our server). Time to swap out that faulty DIMM and get the iLO back in the game!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Using Langchain with Llama**"
      ],
      "metadata": {
        "id": "CpeaKe4IaC8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade langchain\n",
        "#!pip install langchain_community"
      ],
      "metadata": {
        "id": "onCwHggIaCpG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the model first"
      ],
      "metadata": {
        "id": "M7XbT4fWeCAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name = \"GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k-GGUF\"\n",
        "model_file = \"Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf\"\n",
        "\n",
        "# Download the model from Hugging Face\n",
        "model_path = hf_hub_download(repo_id=model_name, filename=model_file)\n",
        "\n",
        "print(f\"Model downloaded to: {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFo_szHSeBMV",
        "outputId": "6b09c94a-4b7e-4050-fda2-4885085d5d50"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded to: /root/.cache/huggingface/hub/models--GPT4All-Community--Meta-Llama-3.1-8B-Instruct-128k-GGUF/snapshots/350b6d7f3a2224c98b6dc8ebdce0e290b71cae22/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# Load the GGUF model\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    temperature=0.2,        # Adjust creativity\n",
        "    max_tokens=128,         # Output length\n",
        "    top_p=0.5,              # Sampling parameter\n",
        "    n_ctx=4096,             # Context window (adjust based on available RAM)\n",
        "    n_threads=4,            # Set based on CPU cores\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZo7uxLieQmC",
        "outputId": "53272404-0c39-4824-dd27-0c5b8c92555c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "template_string = \"\"\"Translate the text that is delimited by triple backticks into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "\n",
        "customer_style = \"\"\"English in a soft tone\"\"\"\n",
        "\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Our server, model GX-780, is experiencing critical downtime. We're seeing memory DIMM errors on slot 4, specifically part number 16GB-DDR4-2933. The iLO is reporting a network connection failure, with a 50% packet loss rate on port 2. We need immediate on-site support to replace the faulty DIMM and troubleshoot the network issue affecting the iLO.\n",
        "\"\"\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "\n",
        "customer_response = llm.invoke(customer_messages)"
      ],
      "metadata": {
        "id": "9pxv8uTJa1On"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAVg8RJUf6kE",
        "outputId": "7f7562f4-44f9-4698-a9fa-183e1cdecb5a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "translation: ```\n",
            "Our server, model GX-780, is currently experiencing a critical shutdown. We're seeing an error with one of the memory modules on slot 4. The part number for this module is 16GB-DDR4-2933. Additionally, our iLO (intelligent management system) is reporting that it's lost connection to the network and is experiencing a high packet loss rate on port 2. We need immediate assistance from an on-site technician to replace the faulty memory module and troubleshoot the network issue affecting the iLO.\n",
            "``` markdown\n",
            "# Server Downtime\n",
            "\n",
            "## Critical Shutdown\n",
            "\n",
            "Our server, model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"Thank you for reporting the server issues on your GX-780. We understand the severity of the downtime. Our technicians are currently investigating the DIMM error on slot 4, part 16GB-DDR4-2933, and the network issues affecting the iLO port 2. We've dispatched a technician to your location with replacement DIMMs. They'll also run diagnostics on the network connectivity to isolate and resolve the packet loss. We'll provide updates every 30 minutes until the server is fully operational.\n",
        "\"\"\"\n",
        "\n",
        "service_style = \"\"\"a teasing and joking tone that speaks in English\"\"\"\n",
        "\n",
        "\n",
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style,\n",
        "    text=service_reply)"
      ],
      "metadata": {
        "id": "7PrAYuj8f6gS"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(service_messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgLPVcRYfeZX",
        "outputId": "8d5abe0a-5650-474f-b994-6c0a61b68deb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is a teasing and joking tone that speaks in English. text: ```Thank you for reporting the server issues on your GX-780. We understand the severity of the downtime. Our technicians are currently investigating the DIMM error on slot 4, part 16GB-DDR4-2933, and the network issues affecting the iLO port 2. We've dispatched a technician to your location with replacement DIMMs. They'll also run diagnostics on the network connectivity to isolate and resolve the packet loss. We'll provide updates every 30 minutes until the server is fully operational.\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_response = llm.invoke(service_messages)"
      ],
      "metadata": {
        "id": "RmpAGCAUgdCX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(service_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNIQ4hgZgc-x",
        "outputId": "a948d2be-0cf6-4185-9aa3-7e488e45713c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the rewritten text in a teasing and joking tone:\n",
            "```\n",
            "Hey there, server superstar! We heard you were having some issues with your trusty GX-780. Don't worry, we've got this!\n",
            "\n",
            "Our tech wizards are on it, investigating the mysterious DIMM error on slot 4 (part 16GB-DDR4-2933 - yeah, that's a mouthful!). And don't even get us started on the network issues affecting the iLO port 2. It's like your server is trying to play a game of \"I'm not listening\"!\n",
            "\n",
            "But fear not, dear server owner! We\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iv8RwLqDgc7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try getting the output in the form of dictionary with this open source model -"
      ],
      "metadata": {
        "id": "2p-moVdSNIQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "\n",
        "\n",
        "# Defines a schema named \"model\". The description instructs the LLM to extract the model name if mentioned in the input text. If no model name is found, it should return \"False\".\n",
        "model_schema = ResponseSchema(name=\"model\",\n",
        "                             description=\"Is any model name specified? Answer the model name directly if yes, False if not or unknown.\")\n",
        "\n",
        "issue_count_schema = ResponseSchema(name=\"issue_count\",\n",
        "                                      description=\"How many issues are reported for the product? If this information is found then give the number, else -1.\")\n",
        "\n",
        "parts_reported_schema = ResponseSchema(name=\"parts_reported\",\n",
        "                                    description=\"This tells about the issues reported in the text? Give all the names. If no part names found then reply with NA.\")\n",
        "\n",
        "\n",
        "# Stores the defined schemas in a list to be used for structured parsing\n",
        "response_schemas = [model_schema,\n",
        "                    issue_count_schema,\n",
        "                    parts_reported_schema]\n",
        "\n",
        "\n",
        "# Creates an instance of StructuredOutputParser, ensuring that the extracted information follows the defined schema\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "\n",
        "# Generates formatting instructions for the LLM to structure its output properly\n",
        "format_instructions = output_parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "ropVNJ2uNhne"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "model: Is any model name specified? Answer the model name directly if yes, False if not or unknown.\n",
        "\n",
        "issue_count: How many issues are reported for the product? If this information is found then give the number, else -1.\n",
        "\n",
        "parts_reported: Tell the issues or the server parts reported? Give all the names of every issues or parts. If no part names found then reply with NA.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review,\n",
        "                                format_instructions=format_instructions)\n"
      ],
      "metadata": {
        "id": "v3ssdwSpihF5"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_4 = llm.invoke(messages)"
      ],
      "metadata": {
        "id": "uRRGlx2yhjOY"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict_2 = output_parser.parse(response_4)"
      ],
      "metadata": {
        "id": "93LTTe9RkmHJ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OFT9sxZkmEo",
        "outputId": "5a847397-6a38-4363-ec96-a8ad8cb05871"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'GX-780',\n",
              " 'issue_count': 2,\n",
              " 'parts_reported': 'memory DIMM, part number 16GB-DDR4-2933'}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict_2.get('issue_count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHDJBKddkmCC",
        "outputId": "8f26c8c9-ec9b-46fe-9c92-4babe1d1b1d7"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iLO is not there in the parts_reported, though it should have been there. Try changing the prompt in the review template as well as the Response schema and see if you can get the iLO as an output too in the parts_reported."
      ],
      "metadata": {
        "id": "H-k2pSlSVAYN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAd7_61rm8Hf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}